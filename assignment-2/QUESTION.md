>1. You need to implement the polygon triangulation problem.  
(a) Prepare a dataset for the convex polygon with increasing number of arbitrary vertices. 
(b) Consider a brute force method where you do an exhaustive search for finding the optimal result.   
(c)  Next apply dynamic programming, in line with the matrix chain multiplication problem.  
(d) Then implement a greedy strategy to solve the same problem by choosing non-intersecting diagonals in sorted order.   
(e)  Finally compare the results and suggest whether the DP and Greedy approach results show mismatch.


>2. Implement data compression strategies:     
(a) Write encoding algorithms for the Shannon-Fano coding scheme.   
(b) Implement Huffman encoding scheme using heap as data structure.    
(c) Implement an instantaneous decoding algorithm   
(d) Choose a large text document and get results of Huffman coding with this document.  
(e) Examine the optimality of Huffman codes as data compression strategy by comparing with another document.

  
>3. We need to scale up the things - from toy problems to real life problems. For this, you need to study some large datasets provided by reputed Universities, like 
(a) SNAP - Stanford Network Analysis Project - datasets collected by Stanford University.    
(b) KONECT - Koblenz Network Collection - datasets compiled by Koblenz University, Deutschland (Germany).     

First study and feel the vastness of these networks from the dynamic and disjoint set operations point of view. Then try to implement graph algorithms like Connected Components and Minimum Spanning Tree using appropriate data structure for these datasets.
